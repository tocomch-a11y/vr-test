<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>Bio-Psychedelic XR (Hand Tracking)</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://unpkg.com/peerjs@1.3.1/dist/peerjs.min.js"></script>
    <style>
        body { margin: 0; background: #000; overflow: hidden; font-family: sans-serif; }
        #overlay { position: absolute; width: 100%; height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center; color: white; background: rgba(0,0,0,0.9); z-index: 100; }
        button { padding: 30px 60px; font-size: 24px; background: linear-gradient(45deg, #00ffff, #ff00ff); border: none; border-radius: 50px; cursor: pointer; font-weight: bold; color: white; box-shadow: 0 0 20px rgba(0,255,255,0.5); }
    </style>
</head>
<body>
    <div id="overlay">
        <div id="status">Wait for Mac Signal (psy-link-stable-mac)</div>
        <button id="startBtn" style="display:none;">HAND-TRACKING START</button>
    </div>
    <audio id="remoteAudio" autoplay></audio>

    <script type="module">
        let scene, camera, renderer, analyzer, dataL, audioCtx;
        let currentMode = 0, lastSwitch = 0, hueBase = 0;
        const groups = Array.from({ length: 12 }, () => new THREE.Group());
        
        const peer = new Peer('psy-link-stable-quest');
        peer.on('call', call => {
            call.answer(new MediaStream());
            call.on('stream', stream => {
                document.getElementById('remoteAudio').srcObject = stream;
                document.getElementById('status').innerText = "Connected!";
                document.getElementById('startBtn').style.display = "block";
                window.remoteStream = stream;
            });
        });

        document.getElementById('startBtn').onclick = () => initXR();

        async function initXR() {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            const source = audioCtx.createMediaStreamSource(window.remoteStream);
            analyzer = audioCtx.createAnalyser();
            analyzer.fftSize = 256;
            source.connect(analyzer);
            dataL = new Uint8Array(analyzer.frequencyBinCount);
            source.connect(audioCtx.destination);
            document.getElementById('overlay').style.display = 'none';

            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ antialias: false, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.xr.enabled = true;
            document.body.appendChild(renderer.domElement);

            setup12Modes();
            groups.forEach(g => { g.visible = false; scene.add(g); });
            groups[0].visible = true;

            navigator.xr.requestSession('immersive-ar', { 
                requiredFeatures: ['local-floor', 'hand-tracking'] 
            }).then(s => renderer.xr.setSession(s));

            renderer.setAnimationLoop(render);
        }

        function setup12Modes() {
            // モードごとの基本オブジェクトを生成（負荷分散のためシンプルに）
            const geos = [
                new THREE.TorusGeometry(10, 0.5, 16, 100), // 0: Ring
                new THREE.IcosahedronGeometry(10, 1),       // 1: Crystal
                new THREE.TorusKnotGeometry(8, 2, 100, 16), // 2: Knot
                new THREE.BoxGeometry(15, 15, 15),          // 3: Cube
                new THREE.OctahedronGeometry(10, 0),        // 4: Octa
                new THREE.SphereGeometry(10, 32, 32),       // 5: Sphere
                new THREE.CylinderGeometry(5, 5, 20, 32),   // 6: Tube
                new THREE.DodecahedronGeometry(10, 0),      // 7: Dodeca
                new THREE.PlaneGeometry(30, 30, 10, 10),    // 8: Mesh
                new THREE.TorusGeometry(10, 3, 8, 6),       // 9: Hex-Ring
                new THREE.ConeGeometry(8, 15, 32),          // 10: Cone
                new THREE.TorusKnotGeometry(8, 0.5, 150, 4) // 11: Spiral
            ];

            geos.forEach((geo, i) => {
                const mat = new THREE.MeshBasicMaterial({ wireframe: true, transparent: true });
                const mesh = new THREE.Mesh(geo, mat);
                groups[i].add(mesh);
                // サブオブジェクトとしてパーティクルなどを追加可能
            });
        }

        function render() {
            const session = renderer.xr.getSession();
            let handDist = 1.0, handSpread = 0.5, handRot = 0;

            if (session) {
                // ハンドトラッキングデータの取得
                for (const inputSource of session.inputSources) {
                    if (inputSource.hand) {
                        const indexTip = inputSource.hand.get('index-finger-tip');
                        const thumbTip = inputSource.hand.get('thumb-tip');
                        const wrist = inputSource.hand.get('wrist');

                        if (indexTip && thumbTip) {
                            // 指の開き具合 (Index to Thumb)
                            const frame = renderer.xr.getFrame();
                            const p1 = frame.getJointPose(indexTip, renderer.xr.getReferenceSpace()).transform.position;
                            const p2 = frame.getJointPose(thumbTip, renderer.xr.getReferenceSpace()).transform.position;
                            handSpread = Math.sqrt((p1.x-p2.x)**2 + (p1.y-p2.y)**2 + (p1.z-p2.z)**2) * 10;
                        }
                        if (wrist) {
                            // 手首の回転
                            const wPose = renderer.xr.getFrame().getJointPose(wrist, renderer.xr.getReferenceSpace());
                            handRot = wPose.transform.orientation.y;
                        }
                    }
                }
            }

            if (analyzer) {
                analyzer.getByteFrequencyData(dataL);
                const low = dataL[4]/255, mid = dataL[15]/255, high = dataL[40]/255;
                hueBase += 0.002 + (high * 0.05);

                // モードのランダム切替 (低音に同期)
                if (low > 0.85 && Date.now() - lastSwitch > 800) {
                    groups[currentMode].visible = false;
                    currentMode = Math.floor(Math.random() * 12);
                    groups[currentMode].visible = true;
                    lastSwitch = Date.now();
                }

                // 視線方向
                const lookVec = new THREE.Vector3(0,0,-1).applyQuaternion(camera.quaternion);

                // カレントモードの演出
                const g = groups[currentMode];
                const m = g.children[0];

                // 1. ハンドトラッキング連動: 指の開きでトゲトゲ化(wireframe密度)やスケール
                m.scale.setScalar(1 + low * 2 + handSpread);
                
                // 2. 手首の回転で色と回転速度
                m.rotation.y += 0.01 + handRot * 0.1;
                m.rotation.x += mid * 0.05;

                // 3. 視線連動: 向いている方向にオブジェクトが少しオフセットされる
                g.position.lerp(lookVec.multiplyScalar(20 + mid * 10), 0.1);
                
                // 4. マテリアル更新 (負荷軽減のため色のみ)
                m.material.color.setHSL((hueBase + handRot)%1, 1, 0.5);
                m.material.opacity = 0.3 + low * 0.7;

                // 空間全体へのLSD効果（常に薄くかける）
                scene.rotation.z = Math.sin(Date.now()*0.0005) * handRot;
            }

            renderer.render(scene, camera);
        }
    </script>
</body>
</html>
